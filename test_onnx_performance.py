#!/usr/bin/env python3
"""
æµ‹è¯• ONNX Runtime YOLOv8 æ¨ç†æ€§èƒ½
ä¸éœ€è¦ç›¸æœºï¼Œä½¿ç”¨éšæœºå›¾åƒæµ‹è¯•
"""

import cv2
import numpy as np
import time
import onnxruntime as ort

print("=" * 60)
print("ONNX Runtime YOLOv8 æ¨ç†æ€§èƒ½æµ‹è¯•")
print("=" * 60)

# æ£€æŸ¥å¯ç”¨çš„ providers
available_providers = ort.get_available_providers()
print(f"\nå¯ç”¨çš„ ONNX Runtime providers: {available_providers}")

if 'ROCMExecutionProvider' in available_providers:
    providers = ['ROCMExecutionProvider', 'CPUExecutionProvider']
    print("âœ… ä½¿ç”¨ ROCMExecutionProvider (AMD GPU åŠ é€Ÿ)")
else:
    providers = ['CPUExecutionProvider']
    print("â„¹ï¸ ä½¿ç”¨ CPUExecutionProvider (CPU ä¼˜åŒ–)")

# åŠ è½½ ONNX æ¨¡å‹
onnx_model_path = 'yolov8n.onnx'
print(f"\nåŠ è½½æ¨¡å‹: {onnx_model_path}")
session = ort.InferenceSession(onnx_model_path, providers=providers)

# è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
input_name = session.get_inputs()[0].name
input_shape = session.get_inputs()[0].shape
output_names = [o.name for o in session.get_outputs()]
print(f"è¾“å…¥: {input_name}, å½¢çŠ¶: {input_shape}")
print(f"è¾“å‡º: {output_names}")

# é…ç½®
YOLO_INPUT_SIZE = 416
NUM_WARMUP = 5
NUM_INFERENCE = 50

print(f"\næ¨ç†é…ç½®:")
print(f"  è¾“å…¥å°ºå¯¸: {YOLO_INPUT_SIZE}x{YOLO_INPUT_SIZE}")
print(f"  é¢„çƒ­æ¬¡æ•°: {NUM_WARMUP}")
print(f"  æµ‹è¯•æ¬¡æ•°: {NUM_INFERENCE}")

# åˆ›å»ºéšæœºæµ‹è¯•å›¾åƒ
test_frame = np.random.randint(0, 255, (1080, 1920, 3), dtype=np.uint8)

def run_onnx_inference_simple(frame, session, input_name, input_size=416):
    """ç®€åŒ–çš„ ONNX æ¨ç†å‡½æ•°"""
    # é¢„å¤„ç†
    img = cv2.resize(frame, (input_size, input_size))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.transpose(2, 0, 1).astype(np.float32) / 255.0
    img = np.expand_dims(img, axis=0)
    
    # æ¨ç†
    outputs = session.run(None, {input_name: img})
    return outputs

# é¢„çƒ­
print("\nğŸ”¥ é¢„çƒ­æ¨¡å‹...")
for i in range(NUM_WARMUP):
    _ = run_onnx_inference_simple(test_frame, session, input_name, YOLO_INPUT_SIZE)
    print(f"  é¢„çƒ­ {i+1}/{NUM_WARMUP}", end='\r')
print("\nâœ… é¢„çƒ­å®Œæˆ")

# æ€§èƒ½æµ‹è¯•
print(f"\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯• ({NUM_INFERENCE} æ¬¡æ¨ç†)...")
inference_times = []

for i in range(NUM_INFERENCE):
    start_time = time.time()
    _ = run_onnx_inference_simple(test_frame, session, input_name, YOLO_INPUT_SIZE)
    inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    inference_times.append(inference_time)
    
    if (i + 1) % 10 == 0:
        avg_time = np.mean(inference_times[-10:])
        print(f"  è¿›åº¦: {i+1}/{NUM_INFERENCE}, æœ€è¿‘10æ¬¡å¹³å‡: {avg_time:.2f}ms ({1000/avg_time:.1f} FPS)")

# ç»Ÿè®¡ç»“æœ
inference_times = np.array(inference_times)
print("\n" + "=" * 60)
print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
print("=" * 60)
print(f"å¹³å‡æ¨ç†æ—¶é—´: {np.mean(inference_times):.2f} ms")
print(f"ä¸­ä½æ•°æ¨ç†æ—¶é—´: {np.median(inference_times):.2f} ms")
print(f"æœ€å°æ¨ç†æ—¶é—´: {np.min(inference_times):.2f} ms")
print(f"æœ€å¤§æ¨ç†æ—¶é—´: {np.max(inference_times):.2f} ms")
print(f"æ ‡å‡†å·®: {np.std(inference_times):.2f} ms")
print(f"\nğŸ¯ ç†è®ºæœ€å¤§ FPS (ä»…æ¨ç†): {1000/np.mean(inference_times):.1f}")
print(f"ğŸ¯ å®é™…é¢„æœŸ FPS (å«é¢„å¤„ç†/æ˜¾ç¤º): {1000/(np.mean(inference_times) + 10):.1f}")
print(f"ğŸ¯ è·³å¸§ä¼˜åŒ–å FPS (SKIP_FRAMES=4): {1000/(np.mean(inference_times)/5 + 10):.1f}")

print("\n" + "=" * 60)
print("æµ‹è¯•å®Œæˆï¼")
print("=" * 60)
